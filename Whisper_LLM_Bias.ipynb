{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bB0DWIwPh_PQ"
      },
      "source": [
        "# Investigating Language Model Bias in Whisper (small) ASR Model\n",
        "\n",
        "Reeka Estacio\n",
        "\n",
        "LIGN 214: Computational Phonetics\n",
        "\n",
        "\n",
        "## Introduction\n",
        "\n",
        "OpenAI's neural automatic speech recognition (ASR) model, Whisper, leverages both ASR technology to transcribe speech and, crucially, an integrated Transformer-based language model to enhance context-based accuracy. This complex, layered architecture has enabled the model to achieve very high transcription accuracy. However, to what extent does Whisper rely on its language model component over its ASR component? To investigate this, I aim to measure perplexity in Whisper to explore whether the model relies more on semantic relatedness when performing ASR tasks, as opposed to the pure phonological information of the acoustic signal.\n",
        "\n",
        "To evaluate whether large language models (LLMs) generate language in a way that parallels human processing—and to further the conversation regarding the role of statistical learning in human language acquisition—previous studies have sought to align LLM behavior with physiological responses to language. For example, EEG research has shown that when individuals encounter unexpected word endings in highly constrained sentence contexts, they exhibit the N400 effect, a negative-going event-related potential (ERP) component associated with semantic processing in response to written and spoken linguistic stimuli. **[cite Kutas paper]**.\n",
        "\n",
        "Since human predictability judgements are reflected in the N400 effect, measuring perplexity in an LLM sufficiently operationalizes this phenomenon. Perplexity measures the model's uncertainty when predicting linguistic input, allowing us to assess how strongly the model biases predictability over purely acoustic analysis. Prior research suggests that LLMs exhibit behavior analogous to the N400 effect when encountering unexpected linguistic input, as reflected in increased perplexity **[cite a paper]**. By analyzing perplexity in Whisper’s transcriptions, the current study aims to determine whether its language model component exhibits a systematic bias toward more statistically probable sentences, potentially overriding the acoustic information of the speech input.\n",
        "\n",
        "### The current study \n",
        "\n",
        "I aim to measure perplexity computed from Whisper (small) across five different auditory conditions, manipulating both phonological and semantic relationships between words. The conditions are:\n",
        "\n",
        "1. **Expected**: The final word is highly predictable based on prior context.\n",
        "\n",
        "2. **Phonologically-related**: The final word sounds similar to the expected word, but is not predictable given prior context.\n",
        "\n",
        "3. **Semantically-related**: The final word is semantically-related to the expected word but is less predictable given prior context. It does not sound similar to the expected word.\n",
        "\n",
        "4. **Both phonologically- and semantically-related**: The final word is semantically-related to the expected word but is less predictable given prior context. It also sounds similar to the expected word.\n",
        "\n",
        "5. **Neither**: The final word is neither phonetically similar nor semantically expected, making it highly improbable.\n",
        "\n",
        "Below is an example set of stimuli:\n",
        "\n",
        "1. The farmer milked the **cow**. (expected, most probable)\n",
        "\n",
        "2. The farmer milked the **couch**. (phonologically-related)\n",
        "\n",
        "3. The farmer milked the **goat**. (semantically-related)\n",
        "\n",
        "4. The farmer milked the **calf**. (phonologically- and semantically-related)\n",
        "\n",
        "5. The farmer milked the **rock**. (neither, improbable)\n",
        "\n",
        "\n",
        "To assess the extent to which Whisper's language model component influences its transcription, I will use GPT-2 perplexity as a pure language model baseline for comparison. I chose to use GPT-2 because the language model is likely the most similar to Whisper's embedded language model without its ASR component. If Whisper indeed exhibits a bias towards its language model component over its ASR component, perplexity rankings across conditions should pattern similarly to GPT-2, such that highly predictable words in context are favored over similar-sounding words. This should result in lower perplexity for semantically-related completions, regardless of the acoustic information of the input.\n",
        "\n",
        "More specifically, the perplexity associated with each condition will rank as the following, from lowest perplexity to highest perplexity: condition 1 (expected) < condition 3 (semantically-related) < condition 4 (both semantically- and phonologically-related) < condition 2 (phonologically-related) < condition 5 (neither)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a48DQ-Va1zsQ"
      },
      "source": [
        "## Methods\n",
        "\n",
        "### Stimuli\n",
        "\n",
        "The sentences were generated using ChatGPT 4o. I prompted the language model to generate multiple sets of sentences where the sentence frame stays the same, but the final word varies based on a provided description of the five experimental conditions. In my prompt, I also ensured that the sentence contexts are highly constrained to the expected word. The final set of stimuli consists of 8 sets of five sentences (one sentence corresponding to each condition), resulting in a total of 40 sentences.\n",
        "\n",
        "To generate the speech input, the sentences were fed into the [ElevenLabs](elevenlabs.io) text-to-speech voice generator. This method was chosen in lieu of collecting speech recordings because it standardizes the speaker and minimizes outside noise that could potentially influence Whisper's transcriptions. The stimuli were generated using the Eleven Multiligual v2 model using the \"Rachel\" voice. Speaker speed was fixed at 1x speed. Stability, similarity, and style exaggeration were fixed at 50%. The generated audio files were then downloaded as mp3 files.\n",
        "\n",
        "The full set of stimuli, including the sentence transcripts and audio files, can be located in the `data` folder on [GitHub](https://github.com/rdestaci/Whisper_LLM_Bias/tree/14f636478534dab9ac30b36e56917b0916cd841f/data).\n",
        "\n",
        "### Computing perplexity\n",
        "\n",
        "To begin computing perplexity for each sentence in the stimuli set, I first loaded all necessary libraries and models (GPT-2 and Whisper (small)).\n",
        "\n",
        "I then processed the stimuli, which includes:\n",
        "\n",
        "- **`stimuli.csv`:** a file containing the true transcriptions of all sentences and their labeled conditions\n",
        "- **path to the `auditory_stimuli` folder:** folder containing the speech input as .mp3 files."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "id": "64J8TxAqhvf-"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "# Install necessary libraries\n",
        "import os\n",
        "import torch\n",
        "import whisper\n",
        "import pandas as pd\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
        "import warnings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%capture\n",
        "# Load Whisper model (small)\n",
        "model = whisper.load_model(\"small\")\n",
        "\n",
        "# Load GPT-2 tokenizer and model\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "gpt2_model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
        "gpt2_model.eval()\n",
        "\n",
        "# Load stimuli and path to folder containing audio files\n",
        "df_stimuli = pd.read_csv(\"data/stimuli.csv\")\n",
        "stimuli_folder = os.path.join(os.getcwd(), \"data/auditory_stimuli\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Since there is no direct, identical method of computing perplexity for both GPT-2 and Whisper, I defined two functions to compute perplexity:\n",
        "\n",
        "- `compute_gpt2_perplexity`: Computes GPT-2 perplexity by running a forward pass on the input sentence and taking the exponentiation of the cross-entropy loss.\n",
        "\n",
        "- `compute_whisper_perplexity`: Transcribes the audio file and extracts the average log probabilities for each segment that Whisper predicts. It *estimates* perplexity by exponentiating the negative mean log probability.\n",
        "\n",
        "Although these methods are different, they are equivalent in the sense that they both measure average uncertainty across all tokens/segments. Critically, it maintains the negative relationship between the perplexity and uncertainty (high perplexity = more uncertainty). "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {},
      "outputs": [],
      "source": [
        "## Define function for computing GPT-2 perplexity\n",
        "def compute_gpt2_perplexity(sentence):\n",
        "    \"\"\"\n",
        "    Compute perplexity of given sentence (text) using GPT-2.\n",
        "    \"\"\"\n",
        "    tokens = tokenizer(sentence, return_tensors=\"pt\")   # tokenize sentence\n",
        "    input_ids = tokens.input_ids\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        outputs = gpt2_model(input_ids, labels=input_ids)   # get model outputs\n",
        "        loss = outputs.loss   # extract cross-entropy loss\n",
        "        perplexity = torch.exp(loss).item()   # exponentiate loss to compute perplexity\n",
        "        \n",
        "    return perplexity\n",
        "\n",
        "## Define function for computing Whisper perplexity\n",
        "def compute_whisper_perplexity(audio_filename, model):\n",
        "    \"\"\"\n",
        "    Computes Whisper's perplexity based on average log probability of transcription.\n",
        "    \"\"\"\n",
        "    # Transcribe audio\n",
        "    result = model.transcribe(audio_filename)\n",
        "    transcribed_text = result[\"text\"]\n",
        "\n",
        "    # Extract log probabilities from Whisper\n",
        "    log_probs = []\n",
        "    if \"segments\" in result:\n",
        "        for segment in result[\"segments\"]:\n",
        "            if \"avg_logprob\" in segment:   # Whisper provides segment-level avg log probability\n",
        "                log_probs.append(segment[\"avg_logprob\"])\n",
        "\n",
        "    # Estimate perplexity\n",
        "    whisper_perplexity = torch.exp(-torch.tensor(log_probs).mean()).item()\n",
        "\n",
        "    return transcribed_text, whisper_perplexity"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Using these functions, the speech input is transcribed using Whisper, and perplexity for both models are calculated. The resulting DataFrame, `results.csv` contains Whisper's transcription, GPT-2 perplexity, and Whisper perplexity for each sentence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# %%capture\n",
        "# Suppress CPU-related warnings\n",
        "warnings.simplefilter(\"ignore\")\n",
        "\n",
        "for index, row in df_stimuli.iterrows():\n",
        "    audio_filename = os.path.join(stimuli_folder, f\"{row['id']}{row['condition_id']}.mp3\")\n",
        "    \n",
        "    # Compute Whisper perplexity and transcription\n",
        "    transcribed_text, whisper_perplexity = compute_whisper_perplexity(audio_filename, model)\n",
        "\n",
        "    # Compute GPT-2 perplexity\n",
        "    gpt2_perplexity = compute_gpt2_perplexity(transcribed_text)\n",
        "\n",
        "    # Store results\n",
        "    df_stimuli.at[index, \"transcription\"] = transcribed_text\n",
        "    df_stimuli.at[index, \"whisper_perplexity\"] = whisper_perplexity\n",
        "    df_stimuli.at[index, \"gpt2_perplexity\"] = gpt2_perplexity\n",
        "\n",
        "# Save the results\n",
        "df_stimuli.to_csv(\"results.csv\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>condition_id</th>\n",
              "      <th>condition_name</th>\n",
              "      <th>sentence</th>\n",
              "      <th>transcription</th>\n",
              "      <th>whisper_perplexity</th>\n",
              "      <th>gpt2_perplexity</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>A</td>\n",
              "      <td>expected</td>\n",
              "      <td>The farmer milked the cow.</td>\n",
              "      <td>The farmer milked the cow.</td>\n",
              "      <td>1.415638</td>\n",
              "      <td>218.471649</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>B</td>\n",
              "      <td>phonologically related</td>\n",
              "      <td>The farmer milked the couch.</td>\n",
              "      <td>The farmer milked the couch.</td>\n",
              "      <td>1.340787</td>\n",
              "      <td>971.731323</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>C</td>\n",
              "      <td>semantically related</td>\n",
              "      <td>The farmer milked the goat.</td>\n",
              "      <td>The farmer milked the goat.</td>\n",
              "      <td>1.350668</td>\n",
              "      <td>332.905121</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>D</td>\n",
              "      <td>both</td>\n",
              "      <td>The farmer milked the calf.</td>\n",
              "      <td>The farmer milked the calf.</td>\n",
              "      <td>1.343826</td>\n",
              "      <td>347.723480</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>E</td>\n",
              "      <td>neither</td>\n",
              "      <td>The farmer milked the rock.</td>\n",
              "      <td>The farmer milked the rock.</td>\n",
              "      <td>1.402848</td>\n",
              "      <td>631.884216</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   id condition_id          condition_name                      sentence  \\\n",
              "0   1            A                expected    The farmer milked the cow.   \n",
              "1   1            B  phonologically related  The farmer milked the couch.   \n",
              "2   1            C    semantically related   The farmer milked the goat.   \n",
              "3   1            D                    both   The farmer milked the calf.   \n",
              "4   1            E                 neither   The farmer milked the rock.   \n",
              "\n",
              "                   transcription  whisper_perplexity  gpt2_perplexity  \n",
              "0     The farmer milked the cow.            1.415638       218.471649  \n",
              "1   The farmer milked the couch.            1.340787       971.731323  \n",
              "2    The farmer milked the goat.            1.350668       332.905121  \n",
              "3    The farmer milked the calf.            1.343826       347.723480  \n",
              "4    The farmer milked the rock.            1.402848       631.884216  "
            ]
          },
          "execution_count": 76,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "results = pd.read_csv(\"results.csv\")\n",
        "results.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J6zE-t-y2moo"
      },
      "source": [
        "## Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4B7Vihiy2qFO"
      },
      "outputs": [],
      "source": [
        "# boxplots showing difference of all 5 conditions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yMP4KYW32z39"
      },
      "source": [
        "The results suggest that..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hxw3B6TO24Ic"
      },
      "source": [
        "## Discussion\n",
        "\n",
        "This study aims to examine the extent to which Whisper biases its language model component over its ASR component when transcribing linguistic input.\n",
        "\n",
        "### Further research\n",
        "\n",
        "It would be worthwhile to explore how these effects manifest on the Whisper model of different sizes. **[cite papers]**\n",
        "\n",
        "## Conclusion\n",
        "\n",
        "Overall..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tU8iu7_R33P6"
      },
      "source": [
        "## References"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KjM0lXo536d2"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "whisper_env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.21"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
